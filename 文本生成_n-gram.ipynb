{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本生成之自动写论文,通过训练[ArXiv](http://arxiv.org/)里面的论文，从而生成一篇新的论文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[语料地址](https://www.kaggle.com/neelshah18/arxivdataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据描述\n",
    "#### Context\n",
    "Collection of 31000+ paper meta data.\n",
    "#### Content\n",
    "This data contains all paper related to ML, CL, NER, AI and CV field publish between 1992 to 2018-Feb.\n",
    "#### Acknowledgements\n",
    "arXiv is open source library for research papers. Thanks to arXiv for spreading knowledge.\n",
    "#### Inspiration\n",
    "To know what research is going on in the computer science all around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过数据集我们可以看到，我们只需要title跟summary部分\n",
    "<img src=\"./picture1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：加载文本，产生语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...</td>\n",
       "      <td>1</td>\n",
       "      <td>1802.00209v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>2</td>\n",
       "      <td>We propose an architecture for VQA which utili...</td>\n",
       "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Dual Recurrent Attention Units for Visual Ques...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'name': 'Ji Young Lee'}, {'name': 'Franck De...</td>\n",
       "      <td>12</td>\n",
       "      <td>1603.03827v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>3</td>\n",
       "      <td>Recent approaches based on artificial neural n...</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Sequential Short-Text Classification with Recu...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...</td>\n",
       "      <td>2</td>\n",
       "      <td>1606.00776v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>6</td>\n",
       "      <td>We introduce the multiresolution recurrent neu...</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Multiresolution Recurrent Neural Networks: An ...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'name': 'Sebastian Ruder'}, {'name': 'Joachi...</td>\n",
       "      <td>23</td>\n",
       "      <td>1705.08142v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>5</td>\n",
       "      <td>Multi-task learning is motivated by the observ...</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
       "      <td>Learning what to share between loosely related...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'name': 'Iulian V. Serban'}, {'name': 'Chinn...</td>\n",
       "      <td>7</td>\n",
       "      <td>1709.02349v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>9</td>\n",
       "      <td>We present MILABOT: a deep reinforcement learn...</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              author  day            id  \\\n",
       "0  [{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...    1  1802.00209v1   \n",
       "1  [{'name': 'Ji Young Lee'}, {'name': 'Franck De...   12  1603.03827v1   \n",
       "2  [{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...    2  1606.00776v2   \n",
       "3  [{'name': 'Sebastian Ruder'}, {'name': 'Joachi...   23  1705.08142v2   \n",
       "4  [{'name': 'Iulian V. Serban'}, {'name': 'Chinn...    7  1709.02349v2   \n",
       "\n",
       "                                                link  month  \\\n",
       "0  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
       "1  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
       "2  [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
       "3  [{'rel': 'alternate', 'href': 'http://arxiv.or...      5   \n",
       "4  [{'rel': 'alternate', 'href': 'http://arxiv.or...      9   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We propose an architecture for VQA which utili...   \n",
       "1  Recent approaches based on artificial neural n...   \n",
       "2  We introduce the multiresolution recurrent neu...   \n",
       "3  Multi-task learning is motivated by the observ...   \n",
       "4  We present MILABOT: a deep reinforcement learn...   \n",
       "\n",
       "                                                 tag  \\\n",
       "0  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
       "1  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
       "2  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
       "3  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
       "4  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
       "\n",
       "                                               title  year  \n",
       "0  Dual Recurrent Attention Units for Visual Ques...  2018  \n",
       "1  Sequential Short-Text Classification with Recu...  2016  \n",
       "2  Multiresolution Recurrent Neural Networks: An ...  2016  \n",
       "3  Learning what to share between loosely related...  2017  \n",
       "4              A Deep Reinforcement Learning Chatbot  2017  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41000\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "wpt = WordPunctTokenizer()\n",
    "process = lambda text: ' '.join(wpt.tokenize(text.lower()))\n",
    "lines = list(map(process,lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['differential contrastive divergence ; this paper has been retracted .',\n",
       " 'what does artificial life tell us about death ? ; short philosophical essay',\n",
       " 'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lines, key=len)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义n-gram函数\n",
    "关于n-gram的解释，这篇文章写的很好https://blog.csdn.net/ahmanz/article/details/51273500  \n",
    "n-gram函数:计算每个单词在前(n-1)个单词出现的次数 \n",
    "这里有两点是需要注意的  \n",
    "- 如果前(n-1)的索引是在句子开头，需要用UNK来填充，例如：n=3  \n",
    "empty prefix: \"\" -> (UNK, UNK)  \n",
    "short prefix: \"the\" -> (UNK, the)  \n",
    "long prefix: \"the new approach\" -> (new, approach)  \n",
    "- 句子结尾加上<EOS>标识，代表句子的结束  \n",
    "\"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines, n):\n",
    "    '''\n",
    "    计算每个单词在前(n-1)个单词出现的次数\n",
    "    :param lines: 空格符分隔的句子列表\n",
    "    :returns: { tuple(前(n-1)个单词): {下一个单词1: count_1, 下一个单词2: count_2}}\n",
    "    '''\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "\n",
    "    for line in lines:\n",
    "        line_list = line.split()\n",
    "        len_line = len(line_list)\n",
    "        for i,word in enumerate(line_list):\n",
    "            key =[]\n",
    "            for j in range(1,n):\n",
    "                if i-j >= 0:\n",
    "                    key.append(line_list[i-j])\n",
    "                else:\n",
    "                    key.append(UNK)\n",
    "            key = tuple(key[::-1])\n",
    "            counts[key][word]+=1\n",
    "            if i == len_line-1:\n",
    "                key2= list(key[1:])\n",
    "                key2.append(word)\n",
    "                key2=tuple(key2)\n",
    "                counts[key2][EOS]+=1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "print(dummy_lines[2])\n",
    "print(dummy_counts[('_UNK_', 'a')]['note'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 13, 'the': 3, 'on': 3, 'using': 2, 'learning': 2, 'automatic': 2, 'why': 2, 'proceedings': 2, 'piecewise': 2, 'differential': 1, 'what': 1, 'p': 1, 'computational': 1, 'weak': 1, 'creating': 1, 'defeasible': 1, 'essence': 1, 'deep': 1, 'statistical': 1, 'complex': 1, 'serious': 1, 'preprocessing': 1, 'liquid': 1, 'mining': 1, 'towards': 1, 'icon': 1, 'recognition': 1, 'glottochronologic': 1, 'utility': 1, 'temporized': 1, 'backpropagation': 1, 'random': 1, 'network': 1, 'glottochronology': 1, 'time': 1, 'convolutional': 1, 'fitness': 1, 'flip': 1, 'autonomous': 1, 'activitynet': 1, 'decision': 1, 'text': 1, 'discrimination': 1, 'are': 1, 'extraction': 1, 'comments': 1, 'resource': 1, 'advances': 1, 'exploration': 1, 'quantified': 1, 'in': 1, 'introduction': 1, 'beyond': 1, 'norm': 1, 'about': 1, 'unary': 1, 'some': 1, 'convex': 1, 'neurocontrol': 1, 'philosophy': 1, 'parallels': 1, 'an': 1, 'calculate': 1, 'group': 1, 'entropy': 1, 'word': 1, 'guarded': 1, 'cornell': 1, 'semistability': 1, 'attack': 1, 'standardization': 1, 'defensive': 1, 'how': 1, 'technical': 1, 'sat': 1, 'approximated': 1, 'solving': 1, 'agent': 1})\n"
     ]
    }
   ],
   "source": [
    "print(dummy_counts[('_UNK_', '_UNK_')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算前(n-1)个单词产生w_t的概率为:\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        '''\n",
    "        训练一个 count-based language model: \n",
    "        计算n-gram的概率：P(w_t | prefix) \n",
    "        \n",
    "        :param n: n-gram\n",
    "        :param lines: 空格符分隔的句子列表\n",
    "        '''\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        \n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        self.probs = defaultdict(Counter)\n",
    "        \n",
    "        # 计算实际的概率，实现上一个单元格表示的公式\n",
    "        for prefix,counts_pre in counts.items():\n",
    "            sum_prefix = sum([value for value in counts_pre.values()])\n",
    "            for word,num in counts_pre.items():\n",
    "                self.probs[prefix][word] = num/sum_prefix\n",
    "            \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        '''\n",
    "        :returns: 所有token的字典：{token : it's probability}\n",
    "        '''\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        '''\n",
    "        给定prefix，获得给定token的概率\n",
    "        :returns: P(next_token|prefix), 0 <= P <= 1\n",
    "        '''\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n",
      "0.13\n"
     ]
    }
   ],
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "\n",
    "print(p_initial['learning'])\n",
    "print(p_initial['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076923076923078\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
    "print(p_a['note'])\n",
    "print(p_a.get('the', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "产生句子的步骤\n",
    "```\n",
    "X=[]  \n",
    "while w_next!= '<EOS>':  \n",
    "    w_next = P(w_next|predix)\n",
    "    X.append(w_next\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让语料库中占比较大的单词sample出来的概率更大，我们需要设定一个temperature，这篇博客有详细的分析\n",
    "https://www.jianshu.com/p/e054cd99089e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_prob_token(probs,temperature):\n",
    "    if temperature<=0.01:\n",
    "        temperature = 0.01\n",
    "    s = sum([value**(1/temperature) for value in probs.values()])\n",
    "    n = random.uniform(0, s)\n",
    "    word = ''\n",
    "    for word, prob in probs.items():\n",
    "        prob = prob ** (1/temperature)\n",
    "        if n<prob:\n",
    "            break\n",
    "        n -= prob\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(lm, prefix, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    token = get_prob_token(lm.get_possible_next_tokens(prefix),temperature)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = NGramLanguageModel(lines, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'been': 9042, 'not': 384, 'also': 179, 'only': 159, 'occurred': 83, 'lately': 79, 'very': 74})\n"
     ]
    }
   ],
   "source": [
    "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
    "print(test_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'been': 9968, 'not': 25, 'also': 3, 'only': 2, 'lately': 2})\n"
     ]
    }
   ],
   "source": [
    "test_freqs = Counter([get_next_token(lm, 'there have',temperature=0.5) for _ in range(10000)])\n",
    "print(test_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就可以用我们的模型生成论文了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello edge : learning canonical appearance transformations applied to genet . finally , the correlations in texts accompanied by rich semantic model to focus on one important problem in visual dialog scenario . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'hello' \n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between the two - stage approach of first - person tracking in real - world applications , such as the number of parameters in the total number of clusters is proposed . a comprehensive study on the other hand , the proposed method is proposed for the first time , we propose a new method for using dynamic programming algorithms under consideration for acceptance in upcoming conferences ; the use of the data . the results of a deep learning methods and show that the proposed method is more robust to the best performance . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'bridging the' \n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算困惑度,介绍https://www.itread01.com/content/1542392710.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
    "    log_perplexity = 0\n",
    "    count_words = 0\n",
    "    for line in lines:\n",
    "        sent = line.split()\n",
    "        prefix = [UNK] * (lm.n - 1)\n",
    "        prob = lm.get_next_token_prob(' '.join(prefix), sent[0])\n",
    "        if prob > 0:\n",
    "            addition = np.log(prob)\n",
    "            if addition < min_logprob:\n",
    "                addition = min_logprob\n",
    "            log_perplexity += addition\n",
    "        else:\n",
    "            log_perplexity += min_logprob\n",
    "\n",
    "        count_words += len(sent) + 1\n",
    "        for i, token in enumerate(sent[:-1]):\n",
    "            prefix = prefix[1:] + [token]\n",
    "            prob = lm.get_next_token_prob(' '.join(prefix), sent[i + 1])\n",
    "            if prob > 0:\n",
    "                addition = np.log(prob)\n",
    "                if addition < min_logprob:\n",
    "                    addition = min_logprob\n",
    "                log_perplexity += addition\n",
    "            else:\n",
    "                log_perplexity += min_logprob\n",
    "            \n",
    "        prefix = prefix[1:] + [sent[-1]]\n",
    "        prob = lm.get_next_token_prob(' '.join(prefix), EOS)\n",
    "        if prob > 0:\n",
    "            addition = np.log(prob)\n",
    "            if addition < min_logprob:\n",
    "                addition = min_logprob\n",
    "            log_perplexity += addition\n",
    "        else:\n",
    "            log_perplexity += min_logprob\n",
    "            \n",
    "    return np.exp(-log_perplexity / count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面代码发现，N-gram模型，遇到未见过的词组，其概率为0，导致困惑度很大，实际上可能因为语料库太小，未见过的词组将来可能会出现，所以我们需要进行一些平滑技术，使得其词组的出现概率大于0，具体的平滑方法可以查看：https://blog.csdn.net/qjf42/article/details/79761786"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 3246.38501\n",
      "N = 2, Perplexity = 85653987.28774\n",
      "N = 3, Perplexity = 61999196259043346743296.00000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拉普拉斯平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel): \n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 966.12894\n",
      "N = 2, Perplexity = 470.48021\n",
      "N = 3, Perplexity = 3679.44765\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
